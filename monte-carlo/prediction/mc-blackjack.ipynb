{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    A simple Monte Carlo agent for learning the state value function in Blackjack.\n",
    "\n",
    "    Attributes:\n",
    "        V (dict): A dictionary that maps state tuples to their estimated values.\n",
    "        returns (dict): A dictionary that collects lists of returns for each state.\n",
    "        states_visited (dict): A dictionary that tracks whether a state has been visited in an episode.\n",
    "        memory (list): A list used to store state and reward for each step of the episode.\n",
    "        gamma (float): The discount factor used in calculating returns.\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=0.99):\n",
    "        \"\"\"\n",
    "        Initializes the Agent object with a discount factor and default state space definitions.\n",
    "\n",
    "        Args:\n",
    "            gamma (float): The discount factor for the Monte Carlo learning, defaults to 0.99.\n",
    "        \"\"\"\n",
    "        self.V = {}\n",
    "        self.sum_space = [i for i in range(4, 22)]\n",
    "        self.dealer_show_card_space = [i for i in range(1, 11)]\n",
    "        self.ace_space = [False, True]\n",
    "        self.action_space = [0, 1]\n",
    "        self.state_space = []\n",
    "        \n",
    "        self.returns = {}\n",
    "        self.states_visited = {}\n",
    "        self.memory = []\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.init_vals()\n",
    "\n",
    "    def init_vals(self):\n",
    "        \"\"\"\n",
    "        Initializes the value function, returns, states visited, and state space for all possible states\n",
    "        in a game of Blackjack.\n",
    "        \"\"\"\n",
    "        for total in self.sum_space:\n",
    "            for card in self.dealer_show_card_space:\n",
    "                for ace in self.ace_space:\n",
    "                    self.V[(total, card, ace)] = 0\n",
    "                    self.returns[(total, card, ace)] = []\n",
    "                    self.states_visited[(total, card, ace)] = 0\n",
    "                    self.state_space.append((total, card, ace))\n",
    "\n",
    "    def policy(self, state):\n",
    "        \"\"\"\n",
    "        Defines the policy under which the agent acts. The policy is simple:\n",
    "        hit if the total is less than 20, otherwise stand.\n",
    "\n",
    "        Args:\n",
    "            state (tuple): The current state tuple (total, dealer's card, has_ace).\n",
    "\n",
    "        Returns:\n",
    "            int: The action to take, where 0 is stand and 1 is hit.\n",
    "        \"\"\"\n",
    "        total, _, _ = state\n",
    "        action = 0 if total >= 20 else 1\n",
    "        return action\n",
    "\n",
    "    def update_V(self):\n",
    "        \"\"\"\n",
    "        Updates the value estimates V for all states based on the returns obtained from completed episodes.\n",
    "        It uses first-visit Monte Carlo method for updating.\n",
    "        \"\"\"\n",
    "        for idt, (state, _) in enumerate(self.memory):\n",
    "            G = 0\n",
    "            if self.states_visited[state] == 0:\n",
    "                self.states_visited[state] += 1\n",
    "                discount = 1  # gamma ^ 0 == 1\n",
    "                for t, (_, reward) in enumerate(self.memory[idt:]):\n",
    "                    G += reward * discount\n",
    "                    discount *= self.gamma\n",
    "                    self.returns[state].append(G)\n",
    "\n",
    "        for state, _ in self.memory:\n",
    "            self.V[state] = np.mean(self.returns[state])\n",
    "\n",
    "        for state in self.state_space:\n",
    "            self.states_visited[state] = 0\n",
    "\n",
    "        self.memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to create the environment, instantiate the agent, and run multiple episodes\n",
    "    to learn the value function.\n",
    "\n",
    "    It uses the `gymnasium` Blackjack environment and runs for a specified number of episodes.\n",
    "    \"\"\"\n",
    "    env = gym.make(\"Blackjack-v1\")\n",
    "    agent = Agent()\n",
    "    n_episodes = 500000\n",
    "\n",
    "    for i in range(n_episodes):\n",
    "        if i % 50000 == 0:\n",
    "            print(\"Starting episode:\", i)\n",
    "\n",
    "        observation, info = env.reset()\n",
    "        terminated, truncated = False, False\n",
    "\n",
    "        while not terminated or truncated:\n",
    "            action = agent.policy(observation)\n",
    "            observation_, reward, terminated, truncated, info = env.step(action)\n",
    "            agent.memory.append((observation, reward))\n",
    "            observation = observation_\n",
    "\n",
    "        agent.update_V()\n",
    "\n",
    "    print(\"Likely Win State:\", agent.V[21, 3, True])\n",
    "    print(\"Likely Lose State:\", agent.V[4, 1, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode: 0\n",
      "Starting episode: 50000\n",
      "Starting episode: 100000\n",
      "Starting episode: 150000\n",
      "Starting episode: 200000\n",
      "Starting episode: 250000\n",
      "Starting episode: 300000\n",
      "Starting episode: 350000\n",
      "Starting episode: 400000\n",
      "Starting episode: 450000\n",
      "Likely Win State: 0.9691252144082333\n",
      "Likely Lose State: -0.18684704286783044\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
